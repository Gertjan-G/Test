{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2590d42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T13:20:50.347963Z",
     "start_time": "2023-05-25T13:20:50.205344500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e6d8c8db6a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "# TOMTOM API instellen\n",
    "API_ENDPOINT = 'https://api.tomtom.com/search/2/geocode/{address}.json'\n",
    "API_PARAMS = {\n",
    "    'key': '8mA6ufG2r6XlxHyI5ojoJRjXsRONZS2x',\n",
    "    'countrySet': 'BE',\n",
    "    'language': 'en-US',\n",
    "}\n",
    "\n",
    "\n",
    "class PageTimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_page_timeout(start_time, timeout=30):\n",
    "    if time.time() - start_time >= timeout:\n",
    "        print(\"te lang op deze pagina\")\n",
    "        raise PageTimeoutError()\n",
    "\n",
    "\n",
    "def initialiseer_chrome():\n",
    "    # Configureer Chrome-opties\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    #chrome_options.add_argument(\"--disable-images\")  # Schakel afbeeldingen uit\n",
    "    #chrome_options.add_argument(\"--disable-extensions\")  # Schakel extensies uit\n",
    "\n",
    "    # CreÃ«er een nieuw exemplaar van de Chrome-browser met de geconfigureerde opties\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    return browser\n",
    "\n",
    "\n",
    "def haal_detail_urls(browser, base_url):\n",
    "    #Haal de detail-URL's van de pagina's.\n",
    "\n",
    "    page_number = 1\n",
    "    detail_urls = []\n",
    "\n",
    "    while True:\n",
    "        print(page_number)\n",
    "        url = base_url.format(page_number)\n",
    "        start_time = time.time()\n",
    "        timeout_thread = threading.Thread(target=check_page_timeout, args=(start_time,))\n",
    "        timeout_thread.start()\n",
    "\n",
    "        try:\n",
    "            browser.get(url)\n",
    "            # Wacht tot de pagina is geladen en de JavaScript is weergegeven.\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                accept_button = browser.execute_script(\n",
    "                    'return document.getElementById(\"usercentrics-root\").shadowRoot.querySelector(\\'[data-testid=\"uc-accept-all-button\"]\\')')\n",
    "                accept_button.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "            items = soup.find_all(\"a\", {\"class\": \"card__title-link\"})\n",
    "\n",
    "            for item in items:\n",
    "                detail_url = item.get(\"href\")\n",
    "                detail_urls.append(detail_url)\n",
    "\n",
    "            next_page_link = soup.find(\"span\", {\"class\": \"sr-only\"}, text=\"Volgende pagina\")\n",
    "            if not next_page_link:\n",
    "                break\n",
    "\n",
    "        except PageTimeoutError:\n",
    "            print(f\"Timeout reached for page {page_number}. Moving to the next page.\")\n",
    "\n",
    "        finally:\n",
    "            timeout_thread.join()  # Stop the timeout thread.\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "    return detail_urls\n",
    "\n",
    "\n",
    "def initialiseer_dataframes():\n",
    "    # Open de CSV bestanden\n",
    "    gebouw = 'data_gebouw.csv'\n",
    "    appartement = 'data_appartement.csv'\n",
    "    #kijk of het bestand bestaad, anders maak je nieuwe dataframe aan.\n",
    "    try:\n",
    "        data_gebouw = pd.read_csv(gebouw, sep=';')\n",
    "    except FileNotFoundError:\n",
    "        columns = [\"typevastgoed\", \"beschrijving\", \"code\", \"adres\", \"laatstgezien\", \"latitude\", \"longitude\"]\n",
    "        data_gebouw = pd.DataFrame(columns=columns)\n",
    "\n",
    "    try:\n",
    "        data_appartement = pd.read_csv(appartement, sep=';')\n",
    "    except FileNotFoundError:\n",
    "        columns = [\"code\", \"prijs\", \"oppervlakte\", \"verdieping\", \"soort\"]\n",
    "        data_appartement = pd.DataFrame(columns=columns)\n",
    "\n",
    "    return data_gebouw, data_appartement\n",
    "\n",
    "\n",
    "def sla_data_op(data_gebouw, data_appartement):\n",
    "    #Sla de dataframes op als csv-bestanden.\n",
    "    data_gebouw.to_csv('data_gebouw.csv', encoding='utf-8', index=False, sep=';')\n",
    "    data_appartement.to_csv('data_appartement.csv', encoding='utf-8', index=False, sep=';')\n",
    "\n",
    "\n",
    "#functie voor het downloaden van de afbeeldingen       \n",
    "def download_image(soup, code):\n",
    "    # Zoek het knopelement dat de afbeelding bevat.\n",
    "    button_element = soup.find(\"button\", {\"class\": \"classified-gallery__button\"})\n",
    "    if button_element:\n",
    "        # Zoek het afbeeldingselement binnen het knopelement en haal de bron-URL ervan op.\n",
    "        preview_picture = button_element.find(\"img\", {\"class\": \"classified-gallery__picture\"})\n",
    "        if preview_picture:\n",
    "            preview_picture_url = preview_picture.get(\"src\")\n",
    "            if preview_picture_url:\n",
    "                # Download de voorbeeldafbeelding en sla deze op als een bestand.\n",
    "                img_path = f\"img/immoweb/{code}.jpg\"\n",
    "                response = requests.get(preview_picture_url)\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Saved image for {code} to {img_path}\")\n",
    "            else:\n",
    "                print(f\"Error: Failed to extract preview picture URL for {code}\")\n",
    "        else:\n",
    "            print(f\"Error: Failed to find preview picture for {code}\")\n",
    "    else:\n",
    "        print(f\"Error: Failed to find button element for {code}\")\n",
    "\n",
    "\n",
    "# Web scraper:\n",
    "def main():\n",
    "    #basis url definieren\n",
    "    base_url = \"https://www.immoweb.be/nl/zoeken/nieuwbouwproject-appartementen/te-koop?countries=BE&page={}&orderBy=relevance\"\n",
    "    #browser functie oproepen\n",
    "    browser = initialiseer_chrome()\n",
    "    detail_urls = haal_detail_urls(browser, base_url)\n",
    "    data_gebouw, data_appartement = initialiseer_dataframes()\n",
    "    #door de lijst met url's lopen en data ophalen.\n",
    "    for detail_url in detail_urls:\n",
    "\n",
    "        # Haal de code op op basis van de url\n",
    "        code = detail_url.split(\"/\")[-1]\n",
    "\n",
    "        # kijk of deze code reeds bestaad, zoja sla deze pagina over.\n",
    "        if (data_gebouw[\"code\"].astype(str) == code).any():\n",
    "            print(code, \"is al aanwezig, sla deze over\")\n",
    "        else:\n",
    "            print('Nieuwe data gevonden: ' + code)\n",
    "\n",
    "            #kijk na of de pagina opent zoals verwacht\n",
    "            try:\n",
    "\n",
    "                # open de pagina\n",
    "                browser.get(detail_url)\n",
    "                time.sleep(5)\n",
    "\n",
    "                # haal de html uit de pagina\n",
    "                soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "                error_message = soup.find('h1')\n",
    "                #sla over als er iets mis is met de pagina\n",
    "                if error_message and (\n",
    "                        'Helaas ging er iets mis.' in error_message.text or '500 server error' in error_message.text):\n",
    "                    print('Error 404, next url ...')\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        # open de pagina\n",
    "                        browser.get(detail_url)\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        # sluit de cookie banner\n",
    "                        try:\n",
    "                            accept_button = browser.execute_script(\n",
    "                                'return document.getElementById(\"usercentrics-root\").shadowRoot.querySelector(\\'[data-testid=\"uc-accept-all-button\"]\\')')\n",
    "                            accept_button.click()\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        html = browser.page_source\n",
    "\n",
    "                        # Parse de HTML-inhoud met BeautifulSoup.\n",
    "                        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                        # vind de gewenste informatie.\n",
    "\n",
    "                        type_vastgoed = \"Nieuwbouwproject Appartementen\"\n",
    "\n",
    "                        try:\n",
    "                            code = soup.find(\"div\", {\"class\": \"classified__header--immoweb-code\"}).text.strip()\n",
    "                        except:\n",
    "                            print(\"Er ging iets mis, volgende link\")\n",
    "                            break\n",
    "\n",
    "                        code = code.replace(\"Immoweb code : \", \"\").strip()\n",
    "                        address = soup.find(\"div\", {\"class\": \"classified__information--address\"}).text.strip()\n",
    "                        address = address.replace(\"â\", \"\").strip()\n",
    "                        address = \" \".join(address.split())\n",
    "\n",
    "                        latitude = \"\"\n",
    "                        longitude = \"\"\n",
    "                        prijs = \"\"\n",
    "                        verdieping = \"\"\n",
    "                        oppervlakte = \"\"\n",
    "                        vedieping = \"\"\n",
    "\n",
    "                        # zoek de beschrijving\n",
    "                        try:\n",
    "                            beschrijving = soup.find(\"p\", {\"class\": \"classified__description\"}).text.strip()\n",
    "\n",
    "                        except AttributeError:\n",
    "                            #Als de beschrijving niet op de gebruikelijke plaats wordt gevonden, zoek deze dan op in het genoemde div-element \"classified-description-content-text\"\n",
    "                            try:\n",
    "                                div_description = soup.find(\"div\", {\"id\": \"classified-description-content-text\"})\n",
    "                                if div_description.find_all(\"p\"):\n",
    "                                    beschrijving = div_description.find_all(\"p\")[0].text.strip()\n",
    "                                else:\n",
    "                                    raise AttributeError\n",
    "                            except AttributeError:\n",
    "                                print(\"Pagina wordt overgeslagen omdat de beschrijving niet kan worden gevonden\")\n",
    "                                continue\n",
    "\n",
    "                        #Geocode - TOMTOM API\n",
    "                        url = API_ENDPOINT.format(address=address)\n",
    "                        response = requests.get(url, params=API_PARAMS)\n",
    "\n",
    "                        if response.ok:\n",
    "                            data_json = response.json()\n",
    "                            if data_json['summary']['numResults'] > 0:\n",
    "                                latitude = data_json['results'][0]['position']['lat']\n",
    "                                longitude = data_json['results'][0]['position']['lon']\n",
    "                                print(\"Address added:\", latitude, \",\", longitude)\n",
    "                            else:\n",
    "                                latitude = ''\n",
    "                                longitude = ''\n",
    "                                print(\"Address not found\")\n",
    "                        else:\n",
    "                            latitude = ''\n",
    "                            longitude = ''\n",
    "                            print(\"Failed API request\")\n",
    "\n",
    "                        new_row_gebouw = {\n",
    "                            \"typevastgoed\": type_vastgoed,\n",
    "                            \"beschrijving\": beschrijving,\n",
    "                            \"code\": code,\n",
    "                            \"adres\": address,\n",
    "                            \"laatstgezien\": datetime.today().strftime('%Y-%m-%d'),\n",
    "                            \"latitude\": latitude,\n",
    "                            \"longitude\": longitude,\n",
    "                        }\n",
    "\n",
    "                        new_row_gebouw = pd.DataFrame([new_row_gebouw])\n",
    "                        data_gebouw = pd.concat([data_gebouw, new_row_gebouw], ignore_index=True)\n",
    "\n",
    "                        # Zoek naar de tabel met appartementdetails.\n",
    "                        table = soup.find(\"ul\", {\"class\": \"classified__list classified__list--striped\"})\n",
    "\n",
    "                        # Definieer de rijen.\n",
    "                        rows = table.find_all(\"li\",\n",
    "                                              {\"class\": \"classified-with-plan__list-item classified__list-item-link\"})\n",
    "                        #print(\"Rows: \", rows)\n",
    "                        # Loop over alle rijen\n",
    "                        for row in rows:\n",
    "                            grid_items = row.find_all(\"p\", {\"class\": \"grid__item\"})\n",
    "\n",
    "                            # Filter relevante rijen\n",
    "                            filtered_grid_items = [grid_item for grid_item in grid_items if\n",
    "                                                   \"classified__list-item-price\" not in grid_item.attrs.get('class',\n",
    "                                                                                                            [])]\n",
    "\n",
    "                            # # Haal de tekst uit de gefilterde rijen en verwijder onnodige spaties.\n",
    "                            data = [grid_item.get_text(strip=True) for grid_item in filtered_grid_items]\n",
    "\n",
    "                            # verwerk deze tekst en voeg ze toe aan een variabele\n",
    "                            soort = ' '.join(data[0].split())\n",
    "                            area = data[1]\n",
    "                            floor = data[2]\n",
    "                            price = data[3]\n",
    "                            price = price.replace(\"â¬\", \"\")\n",
    "                            price = price.replace(\".\", \"\")\n",
    "\n",
    "                            # kijk of het appartement niet \"verkocht\" is.\n",
    "                            if price.lower() != \"verkocht\":\n",
    "                                # Sla de data op in een df\n",
    "                                new_row_appartement = {\n",
    "                                    \"code\": code,\n",
    "                                    \"prijs\": price,\n",
    "                                    \"oppervlakte\": area.replace(\"mÂ²\", \"\"),\n",
    "                                    \"verdieping\": floor,\n",
    "                                    \"soort\": soort\n",
    "                                }\n",
    "\n",
    "                                # Converteer new_row_appartement naar een DataFrame.\n",
    "                                new_row_appartement = pd.DataFrame([new_row_appartement])\n",
    "\n",
    "                                # combineer het data_appartement DataFrame met het new_row_appartement DataFrame.\n",
    "                                data_appartement = pd.concat([data_appartement, new_row_appartement], ignore_index=True)\n",
    "                                #print(\"Apartment Data: \", new_row_appartement)\n",
    "\n",
    "                        # roep de functie op om de afbeelding te downloaden\n",
    "                        download_image(soup, code)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print('Error occurred: ', str(e))\n",
    "                        continue\n",
    "\n",
    "            finally:\n",
    "                #sla de data op ook als er een error zou zijn.\n",
    "                sla_data_op(data_gebouw, data_appartement)\n",
    "                print(\"data opgeslagen\")\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
